# ollama-chat-python
A simple, zero-dependency Python REPL client that connects to your locally-hosted Ollama HTTP server, just pull a model, run ollama serve, and start chatting.
